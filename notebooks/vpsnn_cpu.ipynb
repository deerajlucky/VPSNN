{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "\n",
    "import get_params as params\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from classifier_models.utils import return_model_accuracy\n",
    "from classifier_models.vpnn import (FCMVPCSNN, FCMVPSNN, FCTMVPSNN, FCVPNN,\n",
    "                                    FCVPSNN, FCVPTSNN)\n",
    "from dataloaders.load_dataset import load_ecg_real\n",
    "from nn_layers.vp_layers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params.get('../network_configs/ECG_VPNN.yaml')\n",
    "\n",
    "torch.manual_seed(params.SEED_NUM)\n",
    "random.seed(params.SEED_NUM)\n",
    "\n",
    "data_path = os.path.join('../data', 'ecg', 'mitbih')\n",
    "train_loader, test_loader = load_ecg_real(params.batch_size, data_path)\n",
    "number_of_observations = len(train_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCVPTSNN(\n",
       "  (tdvp): temporal_tdvp_layer()\n",
       "  (tdlinear1): tdLinear(\n",
       "    in_features=4, out_features=8, bias=True\n",
       "    (spike): LIFSpike()\n",
       "  )\n",
       "  (tdlinear2): tdLinear(\n",
       "    in_features=8, out_features=20, bias=False\n",
       "    (spike): LIFSpike()\n",
       "  )\n",
       "  (tdlinear3): tdLinear(\n",
       "    in_features=20, out_features=2, bias=False\n",
       "    (spike): LIFSpike()\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (ReLU): ReLU()\n",
       "  (membrane_output_layer): MembraneOutputLayer()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if params.classifier_name == 'FCVPNN':\n",
    "    '''FCVPNN - setup'''\n",
    "    classifier = FCVPNN(params.vp_latent_dim, params.num_classes,\n",
    "                        device=params.device, penalty=params.L2penalty, init_vp=params.init_vp)\n",
    "elif params.classifier_name == 'FCVPSNN':\n",
    "    '''FCVPSNN - setup'''\n",
    "    init_vp = [0.1, 0]\n",
    "    classifier = FCVPSNN(params.vp_latent_dim, params.num_classes, device=params.device,\n",
    "                         penalty=params.L2penalty, init_vp=init_vp, n_steps=params.n_steps)\n",
    "elif params.classifier_name == 'FCVPTSNN':\n",
    "    '''FCVPTSNN - setup'''\n",
    "    init_vp = [0.001, 0.001]\n",
    "    classifier = FCVPTSNN(params.vp_latent_dim, params.num_classes, device=params.device,\n",
    "                          penalty=params.L2penalty, init_vp=init_vp, n_steps=params.n_steps)\n",
    "elif params.classifier_name == 'FCMVPSNN':\n",
    "    '''FCMVPSNN - setup'''\n",
    "    init_mvp = [random.random() for i in range(2 * params.n_steps)]\n",
    "    classifier = FCMVPSNN(params.vp_latent_dim, params.num_classes, device=params.device,\n",
    "                          penalty=params.L2penalty, init_vp=init_mvp, n_steps=params.n_steps, m_vp=params.n_steps)\n",
    "elif params.classifier_name == 'FCTMVPSNN':\n",
    "    '''FCTMVPSNN - setup'''\n",
    "    init_mvp = [random.random()/2 for i in range(2 * params.n_steps)]\n",
    "    classifier = FCTMVPSNN(params.vp_latent_dim, params.num_classes, device=params.device,\n",
    "                           penalty=params.L2penalty, init_vp=init_mvp, n_steps=params.n_steps, m_vp=params.n_steps)\n",
    "elif params.classifier_name == 'FCMVPCSNN':\n",
    "    '''FCMVPCSNN - setup'''\n",
    "    init_mvp = [random.random() for i in range(2 * params.n_steps)]\n",
    "    # insert some weights which prooved to be suitable for ECG classification\n",
    "    init_mvp[0:2] = [0.1, 0.0]\n",
    "    classifier = FCMVPCSNN(params.vp_latent_dim, params.num_classes, device=params.device,\n",
    "                           penalty=params.L2penalty, init_vp=init_mvp, n_steps=params.n_steps, m_vp=params.n_steps)\n",
    "\n",
    "classifier.to(params.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define objective and optimizer functions\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adagrad(classifier.parameters(), \n",
    "                       lr=params.lr,\n",
    "                       lr_decay=params.lr_decay,\n",
    "                       weight_decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1], Train accuracy: 58.72%, Test accuracy: 63.90%\n",
      "[Epoch: 2], Train accuracy: 65.38%, Test accuracy: 64.24%\n",
      "[Epoch: 3], Train accuracy: 67.72%, Test accuracy: 65.03%\n",
      "[Epoch: 4], Train accuracy: 71.08%, Test accuracy: 77.07%\n",
      "[Epoch: 5], Train accuracy: 79.71%, Test accuracy: 84.80%\n",
      "[Epoch: 6], Train accuracy: 79.94%, Test accuracy: 86.01%\n",
      "[Epoch: 7], Train accuracy: 80.82%, Test accuracy: 86.61%\n",
      "[Epoch: 8], Train accuracy: 81.27%, Test accuracy: 86.83%\n",
      "[Epoch: 9], Train accuracy: 81.37%, Test accuracy: 87.13%\n",
      "[Epoch: 10], Train accuracy: 81.67%, Test accuracy: 87.42%\n",
      "[Epoch: 11], Train accuracy: 81.70%, Test accuracy: 87.80%\n",
      "[Epoch: 12], Train accuracy: 81.87%, Test accuracy: 88.23%\n",
      "[Epoch: 13], Train accuracy: 82.14%, Test accuracy: 88.45%\n",
      "[Epoch: 14], Train accuracy: 82.15%, Test accuracy: 88.80%\n",
      "[Epoch: 15], Train accuracy: 82.32%, Test accuracy: 89.15%\n",
      "[Epoch: 16], Train accuracy: 82.30%, Test accuracy: 89.61%\n",
      "[Epoch: 17], Train accuracy: 82.38%, Test accuracy: 89.80%\n",
      "[Epoch: 18], Train accuracy: 82.43%, Test accuracy: 90.06%\n",
      "[Epoch: 19], Train accuracy: 82.44%, Test accuracy: 90.33%\n",
      "[Epoch: 20], Train accuracy: 82.42%, Test accuracy: 90.34%\n",
      "[Epoch: 21], Train accuracy: 83.09%, Test accuracy: 90.50%\n",
      "[Epoch: 22], Train accuracy: 82.38%, Test accuracy: 90.75%\n",
      "[Epoch: 23], Train accuracy: 82.19%, Test accuracy: 91.01%\n",
      "[Epoch: 24], Train accuracy: 81.94%, Test accuracy: 91.16%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m classifier(inputs)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over the dataset multiple times\n",
    "for epoch in range(params.epochs_cls):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(params.device), data[1].to(params.device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # print epoch performance\n",
    "    with torch.no_grad():\n",
    "        train_acc = return_model_accuracy(classifier, train_loader, params.device)\n",
    "        test_acc = return_model_accuracy(classifier, test_loader, params.device)\n",
    "        print(f'[Epoch: {epoch + 1}], Train accuracy: {train_acc:.2f}%, Test accuracy: {test_acc:.2f}%')\n",
    "        print(f'[Epoch: {epoch + 1}], Train accuracy: {train_acc:.2f}%, Test accuracy: {test_acc:.2f}%', file=open('output.txt', 'a'))\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
